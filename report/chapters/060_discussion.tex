\section{Discussion}

Majority of us experienced problems in engaging open source or peer-production community. Especially lack of the documentation and set up of these systems was caused as problematic. A standard, proper, detailed documentation  would help overcome onboarding problems. One of the reasons could be  that many open source projects have multiple platforms for members to communicate with each other - forums, the wiki, the mailing lists, and the issue tracker.

However, all of these are separate systems with their own login info and digging into these huge mines and getting required information seems to be a major obstacle. If all the important information like ``Where to start and How to contribute to the project" and ``trouble shooting areas" from these systems had been put in single place, the newbie would have found it easier to take his first steps in the journey of open source contribution. Indeed, some of the members of this class are contributing in making documentation more comprehensive and organized.

Nevertheless these results might be related to another factor too: most of us are not senior developers. Previous studies have highlighted that junior developers experience certain problems when working on larger projects \cite{Begel2008}. This does not invalidate our experiences, but rather highlights questions related to novices and newbies in open source environment. We have touched on question of diversity, and suggested that open source participants are ``mostly middle- to upperclass males who are mostly from developed countries''. Further studies could examine if there is link between diversity, knowledge of development and participation in open collaboration. Also one might ask, if the organization and leadership model -- which we experienced as benevolent -- supports this observation.

% Second aspect which we explored was the business models and outcomes on these choices. We see a link between the choice of licenses and income of revenue. However, our experiences suggest that licensing question is not relevant for participants, rather it seems we focus on skill accusition and interests.

\subsection{Limitations}
\label{sec:limitations}
Our survey sample is small ($n=24$), and therefore the statistical analysis should be considered more indicatory than rigorous. As several students have participated in the same projects, those projects over represented in our data. This implies our data is skewed towards these projects, especially peerlibrary ($n=5$) and hypothes.is ($n=3$).

After reviewing the results in the class, the survey was found to be inadequate in several ways. First, it was biased towards software development, even though not all respondents were involved in software development projects. Secondly, some questions were found difficult, such as the licensing question (\#nn), which left out important licenses such as the Apache license, and number of female contributors (\#nn), which was identified as difficult to answer. We attribute these problems to the rather chaotic process of contributing and lack of quality control.  Many of these problems were already discussed in the previous efforts of academic collaboration \cite{Tomlinson2012}. I suggest that in future, instead of circa 20 participants editing the survey, a benevolent dictator be chosen to coordinate the collaboration effort.

\subsection{Collaboratively writing a paper}

In the beginning of the class we read a paper \cite{Tomlinson2012} in which 30 authors wrote a paper about the process of writing a paper with 30 authors. This paper was largely an experiment of sorts surrounding things we had been learning in class, it was a chance for us to test out new theories of collaborative work.
We think much of that needs to be questioned. There is most definitely room for improvement, but the experiment was extremely informative with regards to figuring out best practices for collaboratively authoring papers with multiple people.  
The process raised a number of interesting questions.
Would this report have been successfully written if we weren't taking this class for a grade?
What tools would have made this project easier? 
Is what has been written an accurate representation of the groups opinions and thoughts? 
What structure is needed to ensure good results?
How can we achieve a consistent tone and style? 
Will we produce anything of value?
At first the process was frustrating, the authoring tools we chose (GitHub and LaTeX) were poorly suited towards collaborative writing. It was difficult to give feedback on other peoples sections and compile the full paper to see where our sections fit in context. In addition, changes did not happen in real time. Making a contribution, the writer had a difficult time answering questions such as: Will it fit? Will it be meaningful? Will it exist in a flow of thought? All of this makes us wonder how far we need to come with collaborative writing and coding tools. There needs to be a better vantage point of what others are doing and what needs to be done. GitHub provides some of this, but it's not in real time. Google docs are in real time, but it is uncontrolled chaos if 30 people are writing. We wonder what would a system in the middle look like? 
Another difficult hurdle to the process of authoring this paper was fitting together paragraphs written by different authors with very different writing styles and levels of participation. As mentioned previously, a lot of the content of this paper was authored in a vacuum to the content and ideas of other authors. A number of sections subgroups delegated paragraph work by creating an outline of ideas. However, as paragraphs came together to form the section, each paragraph addressed the ideas from the outline with differing voices, perspectives, uses of survey results, and citations of academic papers. Fixing this required motivated participants to sweep through the section, rewriting portions of work to create a semblance of clearly flowing ideas.

\subsection{The lack of structure}
Even more challenging was the leaderless nature of the project. As a result, there was very little structure aside from the class. We learned it is hard to design a research agenda with many people. 
Surprisingly, it wasn't always hopeless. We did get work done if we could keep people interested in the project, and even though we were leaderless, leaders eventually emerged. In many ways this was the best kind of experiment: learning by doing.  
We found the process surrounding the production of this report quite challenging and confusing. There's an argument to be made that this is what participating in an open-source project feels like. It is just part of the package. And true, that is often the case. But there are some important differences between an open-source software or editorial project and this report. First of all, prose writing is in general much less easily modularized than code. Other editorial projects make up for this with tools well suited to the job and/or not trying to write one large document that is coherent across its sections. But in this case we did neither. 
Someone who understands editing and the editorial coordination process needs to be steering the ship for a collaboration of 20+ writers to work. However, upon thinking it through more, there's another aspect of the problem that feels even more important: we've been trying to treat this report like an open-source project, but the activities we're undertaking are only partly motivated the same way as open-source participation. We've chosen to take the class, but we are being graded/given course credit for our activities. n a regular open-source project, one can withdraw when things become confusing or chaotic, or only commit to things that feel do-able in the requested timeframe. 
In class, though, if you want to be sure of passing, you can't reasonably opt out of the required work. There is a level of coercion inherent in the institution. (I mean literally, in the institution. Pressure does not come the instructors. Our instructors have to grade us even if they would prefer not to.) Our recommendation to address this problem in future iterations of the class is to change the grading rubric so that students can have more freedom to choose how they participate in class activities. Rather than having a set percentage of a grade derived from the each of report, the blog posts, and contribution to an outside project, percentages should be more flexible based on where a student chooses to put her efforts. 
The areas of contribution should be: contribution to an open collaboration community, reporting on the experience (the blog posts), and producing the collaborative report; each student should be required to participate in two out of the three but not all. Furthermore, the class itself should be considered an open collaboration community that could be joined. Contributions could include actively hashing out governance issues, facilitating discussions, finding helpful readings on topics that come up but are not provided for in the syllabus, and taking on community management tasks that are a part of any well-run open collaboration project. 

\subsubsection{Discussion on Process}
One of the biggest hurdles that seem to come about the process of working on this survey and corresponding report primarily involves organization and clarity on responsibilities and roles. When having so many people contribute to a survey at once without any sort of restraint or control, there seems to have been a lot of redundant questions or questions that overlap with so many others. It was good that we ultimately chose someone to organize the content, but the process and the hurdles felt prolonged and slightly unorganized. Ultimately this does feel like it reflects an open source organization in the fact that there are mass amounts of contributions that required one benevolent dictator to organize and maintain quality control of the content. If it weren't for someone's intervention to organize the survey, it would have been much more of a chore to fill out and analyse. 
The final result of the survey came about easily enough. Google Survey assisted in making the results easy to digest and straightforward to discuss in a report. However, the discussion and exercises to understand what to do with this data in a report involved more discussion in a democratic way that once again, felt a little slow and a bit muddling. So many ideas and proposals on how to go about writing this report came up that it became easy to get lost about what direction everyone is actually taking. Once again I feel like this came about because there was a high emphasis on trying to get everyoneâ€™s input and thoughts in, leading to what felt like a really circular discussion. While I see the value in trying to get everyone's thoughts in the process, I felt as if there should be a stronger leading figure to ease along the final decision making process. 
The exercises in writing the modular parts of this paper were useful though, as well as the ultimate structure of how this report is written. Breaking off into groups to write one large paper, with each smaller group having a leadership figure to oversee the organization and writing process. This seems especially effective due to the fact that one large report can now be written with smaller parts contributed by everyone, and then organized by those responsible for each chapter. The only flaw I see in this is that it may possibly lead to a large paper that has stylistic inconsistencies or unorganized thoughts between chapters. The responsibility of going over the entire report and cleaning the content is a large role and hopefully can be done with ease.
